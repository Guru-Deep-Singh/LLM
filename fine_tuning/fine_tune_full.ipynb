{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8f12c6-cb40-45b6-85e1-46fe9bb68968",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358490f-5eeb-4872-a7ad-007aeac9eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2bbdb8-a7c6-49ee-9e1a-e3915d3e3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import wandb\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34752c6f-6efc-4d93-848e-4128de5d7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14abd15d-c7a4-41d9-ba83-e164d663f4f5",
   "metadata": {},
   "source": [
    "# Constants for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ff378-4d48-42a6-a49a-3f537b32b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-1B\"\n",
    "PROJECT_NAME = \"pricer_guru_llama_3.2-1B-local-full\"\n",
    "HF_USER = \"biggestFudge\"\n",
    "\n",
    "# Data\n",
    "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
    "MAX_SEQUENCE_LENGTH = 182\n",
    "\n",
    "# Run name for saving the model in the hub\n",
    "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "\n",
    "# Hyperparameters for QLoRA\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "LORA_DROPOUT = 0.1\n",
    "QUANT_4_BIT = True\n",
    "\n",
    "# Hyperparameters for Training\n",
    "EPOCHS = 1 \n",
    "BATCH_SIZE = 3 # More VRAM you have, more you can put\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_SCHEDULER_TYPE = 'cosine'\n",
    "WARMUP_RATIO = 0.03\n",
    "OPTIMIZER = \"paged_adamw_32bit\"\n",
    "\n",
    "# Admin config - SAVE_STEPS is how often it will upload to the hub\n",
    "STEPS = 1\n",
    "SAVE_STEPS = 2000\n",
    "LOG_TO_WANDB = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c75a6-1883-44ca-90ef-ecda086833fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUB_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043de1d-bfa2-4987-bd71-f23a45438bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e3e79-06cc-4915-bc4b-700adc4f70a0",
   "metadata": {},
   "source": [
    "# Weights and Biases\n",
    "We will track the training on weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3c81b-78a6-4a63-a77a-1b101da41894",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_api_key = os.environ['WANDB_API_KEY']\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "wandb.login()\n",
    "\n",
    "# Configure Weights & Biases to record against our project\n",
    "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350693b-c457-4166-a6b3-a1b05245d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_WANDB:\n",
    "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa55410-d70b-4e7b-8d95-3bf6c3ef3582",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ef8e1-1a32-41a2-9530-d2d6976f281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb0c7c-fc1d-473a-a7d4-9030d5280bdb",
   "metadata": {},
   "source": [
    "# Quantization Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3eac9a-2b01-482c-bf5b-940d74826006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the right quantization\n",
    "\n",
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf09f83-533f-4bb4-9aeb-33024fa16cc8",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431251c-106e-4bb7-8ab0-007b95aa4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tokenizer and the Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    #torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65868b-278d-4fed-803f-f1ce8afbb152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "response_template = \"Price is $\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489cf88-2893-438a-aa1a-1d8a7a3d9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, specify the configuration parameters for LoRA\n",
    "\n",
    "lora_parameters = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# Next, specify the general configuration parameters for training\n",
    "\n",
    "train_parameters = SFTConfig(\n",
    "    output_dir=PROJECT_RUN_NAME,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_strategy=\"no\",\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    optim=OPTIMIZER,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=10,\n",
    "    logging_steps=STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
    "    run_name=RUN_NAME,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    save_strategy=\"steps\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HUB_MODEL_NAME,\n",
    "    hub_private_repo=True,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "# And now, the Supervised Fine Tuning Trainer will carry out the fine-tuning\n",
    "# Given these 2 sets of configuration parameters\n",
    "# The latest version of trl is showing a warning about labels - please ignore this warning\n",
    "\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train,\n",
    "    peft_config=lora_parameters,\n",
    "    args=train_parameters,\n",
    "    data_collator=collator\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb6c34-dbd3-4007-801b-691e01794ed2",
   "metadata": {},
   "source": [
    "# Starting the Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc143ff7-fc1c-4233-8ffd-e5511268a48b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune!\n",
    "fine_tuning.train()\n",
    "\n",
    "# Push our fine-tuned model to Hugging Face\n",
    "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
    "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4818f-ec2f-42e1-8f61-95350d43c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_WANDB:\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c99b9-8e45-4bb6-8f19-bb6a6ab64e1e",
   "metadata": {},
   "source": [
    "# Testing the Fine-Tuned Model!\n",
    "\n",
    "## Use the model in inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b597f-339d-4a6f-af98-e9e9d771e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = \"2025-07-30_12.37.15\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "REVISION = \"6bf99f872f076148a0b3312426264d998eacc74f\" # or REVISION = None\n",
    "FINETUNED_MODEL = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "\n",
    "\n",
    "# Data\n",
    "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
    "\n",
    "# Hyperparameters for QLoRA\n",
    "QUANT_4_BIT = True\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Used for writing to output in color\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "RESET = \"\\033[0m\"\n",
    "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980ec21-b9c1-4514-b2fa-b409d33d78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "# Load the fine-tuned model with PEFT\n",
    "if REVISION:\n",
    "  fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL, revision=REVISION)\n",
    "else:\n",
    "  fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0bada-71ce-4051-a170-f3697e9c9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if want to see the model's layers\n",
    "#fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4de9f-f0aa-461b-85ab-c7695cfa923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(s):\n",
    "    if \"Price is $\" in s:\n",
    "      contents = s.split(\"Price is $\")[1]\n",
    "      contents = contents.replace(',','')\n",
    "      match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", contents)\n",
    "      return float(match.group()) if match else 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c06de-4625-485c-9823-e3c66efd8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(prompt):\n",
    "    set_seed(42)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
    "    outputs = fine_tuned_model.generate(inputs, attention_mask=attention_mask, max_new_tokens=3, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    return extract_price(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec6348-d6b9-4ceb-bcf0-fe3ae0d23091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An improved prediction function takes a weighted average of the top 3 choices\n",
    "# This code would be more complex if we couldn't take advantage of the fact\n",
    "# That Llama generates 1 token for any 3 digit number\n",
    "import torch.nn.functional as F\n",
    "top_K = 3\n",
    "\n",
    "def improved_model_predict(prompt, device=\"cuda\"):\n",
    "    set_seed(42)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones(inputs.shape, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model(inputs, attention_mask=attention_mask)\n",
    "        next_token_logits = outputs.logits[:, -1, :].to('cpu')\n",
    "\n",
    "    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "    top_prob, top_token_id = next_token_probs.topk(top_K)\n",
    "    prices, weights = [], []\n",
    "    for i in range(top_K):\n",
    "      predicted_token = tokenizer.decode(top_token_id[0][i])\n",
    "      probability = top_prob[0][i]\n",
    "      try:\n",
    "        result = float(predicted_token)\n",
    "      except ValueError as e:\n",
    "        result = 0.0\n",
    "      if result > 0:\n",
    "        prices.append(result)\n",
    "        weights.append(probability)\n",
    "    if not prices:\n",
    "      return 0.0, 0.0\n",
    "    total = sum(weights)\n",
    "    weighted_prices = [price * weight / total for price, weight in zip(prices, weights)]\n",
    "    return sum(weighted_prices).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7d1b7-33d2-4cd2-8672-b6dc2d841904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "\n",
    "    def __init__(self, predictor, data, title=None, size=250):\n",
    "        self.predictor = predictor\n",
    "        self.data = data\n",
    "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
    "        self.size = size\n",
    "        self.guesses = []\n",
    "        self.truths = []\n",
    "        self.errors = []\n",
    "        self.sles = []\n",
    "        self.colors = []\n",
    "\n",
    "    def color_for(self, error, truth):\n",
    "        if error<40 or error/truth < 0.2:\n",
    "            return \"green\"\n",
    "        elif error<80 or error/truth < 0.4:\n",
    "            return \"orange\"\n",
    "        else:\n",
    "            return \"red\"\n",
    "\n",
    "    def run_datapoint(self, i):\n",
    "        datapoint = self.data[i]\n",
    "        guess = self.predictor(datapoint[\"text\"])\n",
    "        truth = datapoint[\"price\"]\n",
    "        error = abs(guess - truth)\n",
    "        log_error = math.log(truth+1) - math.log(guess+1)\n",
    "        sle = log_error ** 2\n",
    "        color = self.color_for(error, truth)\n",
    "        title = datapoint[\"text\"].split(\"\\n\\n\")[1][:20] + \"...\"\n",
    "        self.guesses.append(guess)\n",
    "        self.truths.append(truth)\n",
    "        self.errors.append(error)\n",
    "        self.sles.append(sle)\n",
    "        self.colors.append(color)\n",
    "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
    "\n",
    "    def chart(self, title):\n",
    "        max_error = max(self.errors)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_val = max(max(self.truths), max(self.guesses))\n",
    "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
    "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Model Estimate')\n",
    "        plt.xlim(0, max_val)\n",
    "        plt.ylim(0, max_val)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def report(self):\n",
    "        average_error = sum(self.errors) / self.size\n",
    "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
    "        hits = sum(1 for color in self.colors if color==\"green\")\n",
    "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
    "        self.chart(title)\n",
    "\n",
    "    def run(self):\n",
    "        self.error = 0\n",
    "        for i in range(self.size):\n",
    "            self.run_datapoint(i)\n",
    "        self.report()\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, function, data):\n",
    "        cls(function, data).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f317a-4079-4d4f-a26a-9db5df5970e3",
   "metadata": {},
   "source": [
    "# Testing the model with the Tester harness\n",
    "![fine_tuned_model](../images/fine_tuning_results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64663599-5128-4445-ab18-6695d4776b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(improved_model_predict, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
