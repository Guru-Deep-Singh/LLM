{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469b10f-baeb-43a2-b95a-a269fdf1558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import wandb\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6a904-6eae-4e3e-8d3f-ed821b3998b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc933dd1-3c9b-45cc-9dbe-bf58719d25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-1B\"\n",
    "PROJECT_NAME = \"pricer_guru_llama_3.2-1B-local-full\"\n",
    "HF_USER = \"biggestFudge\" \n",
    "\n",
    "# Data\n",
    "\n",
    "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
    "ORIGINAL_FINETUNED = f\"{HF_USER}/{PROJECT_NAME}-2025-07-30_12.37.15\"\n",
    "ORIGINAL_REVISION = \"6bf99f872f076148a0b3312426264d998eacc74f\"\n",
    "\n",
    "# Run name for saving the model in the hub\n",
    "\n",
    "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "\n",
    "# Hyperparameters for QLoRA\n",
    "\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "LORA_DROPOUT = 0.1\n",
    "QUANT_4_BIT = True\n",
    "\n",
    "# Hyperparameters for Training\n",
    "\n",
    "EPOCHS = 1 # \n",
    "BATCH_SIZE = 2 \n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LEARNING_RATE = 9e-5 #1e-4\n",
    "LR_SCHEDULER_TYPE = 'cosine'\n",
    "WEIGHT_DECAY = 0.001\n",
    "OPTIMIZER = \"paged_adamw_32bit\"\n",
    "MAX_SEQUENCE_LENGTH = 182\n",
    "\n",
    "# Admin config - note that SAVE_STEPS is how often it will upload to the hub\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "STEPS = 50\n",
    "SAVE_STEPS = 2000\n",
    "LOG_TO_WANDB = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f89903-c80c-4151-aa5f-32afa3200f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93eb2f-152b-4e8e-826e-7131fa4eaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_api_key = os.environ['WANDB_API_KEY']\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "wandb.login()\n",
    "\n",
    "# Configure Weights & Biases to record against our project\n",
    "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc347b4-f4d9-4fa0-87c9-492630f3f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a94a8-fa4a-4dda-a68e-6fb0630bb631",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd1abb-3b27-4ab3-a737-068581e39a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_WANDB:\n",
    "  details = {\n",
    "      \"base_model\": BASE_MODEL,\n",
    "      \"dataset\": DATASET_NAME,\n",
    "      \"size\": len(train),\n",
    "      \"epochs\": EPOCHS,\n",
    "      \"alpha\": LORA_ALPHA,\n",
    "      \"r\": LORA_R,\n",
    "      \"dropout\": LORA_DROPOUT,\n",
    "      \"batch_size\": BATCH_SIZE,\n",
    "      \"gradient_accumulation\": GRADIENT_ACCUMULATION_STEPS,\n",
    "      \"lr\": LEARNING_RATE,\n",
    "      \"lr_scheduler\": LR_SCHEDULER_TYPE,\n",
    "      \"max_sequence_length\": MAX_SEQUENCE_LENGTH,\n",
    "      \"quant_4_bit\": QUANT_4_BIT\n",
    "  }\n",
    "\n",
    "  wandb.init(\n",
    "      project=PROJECT_NAME,\n",
    "      name=RUN_NAME,\n",
    "      config=details\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6a159d-5a7e-45b4-be13-df990ba4f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the right quantization\n",
    "\n",
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661b3a3-f37f-4b9b-8642-69523ad362b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tokenizer and the Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")\n",
    "\n",
    "# Load the Tokenizer and the Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load the fine-tuned model with PEFT\n",
    "if ORIGINAL_REVISION:\n",
    "  fine_tuned_model = PeftModel.from_pretrained(base_model, ORIGINAL_FINETUNED, revision=ORIGINAL_REVISION, is_trainable=True)\n",
    "else:\n",
    "  fine_tuned_model = PeftModel.from_pretrained(base_model, ORIGINAL_FINETUNED, is_trainable=True)\n",
    "\n",
    "fine_tuned_model.train()\n",
    "\n",
    "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4748153-6630-48f6-a26f-22e54d4a498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "response_template = \"Price is $\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f5d0b-5f97-4355-9d89-505d0d171589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, specify the configuration parameters for LoRA\n",
    "\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# Next, specify the general configuration parameters for training\n",
    "\n",
    "train_params = SFTConfig(\n",
    "    output_dir=PROJECT_RUN_NAME,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_strategy=\"no\",\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=10,\n",
    "    logging_steps=STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
    "    run_name=RUN_NAME,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    save_strategy=\"steps\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HUB_MODEL_NAME,\n",
    "    hub_private_repo=True,\n",
    ")\n",
    "\n",
    "# And now, the Supervised Fine Tuning Trainer will carry out the fine-tuning\n",
    "# Given these 2 sets of configuration parameters\n",
    "\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=fine_tuned_model,\n",
    "    train_dataset=train,\n",
    "    peft_config=peft_parameters,\n",
    "    args=train_params,\n",
    "    data_collator=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70388a4b-bdc6-44bd-a09b-ece479d55e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune!\n",
    "fine_tuning.train()\n",
    "\n",
    "# Push our fine-tuned model to Hugging Face\n",
    "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
    "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874bc233-0323-41c9-b112-f7c27b05a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_WANDB:\n",
    "  wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
