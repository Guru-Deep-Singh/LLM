{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a4e2ee-1fb9-4afa-b38d-96326d692ef5",
   "metadata": {},
   "source": [
    "# Digital Guru\n",
    "In this notebook we are gonna fine tune an opensource LLM on our own data for it to learn the user's behaviour and tone.\n",
    "\n",
    "Steps:\n",
    "* **Data Curation:** In this section we will parse the whatapp and Instagram data from a csv file. We will clean it for use and upload it to Huggingface.\n",
    "  Note: For whatsapp csv I used imazing to export the csv files but for instagram I used meta's official data exporter. The instagram data is exported as HTML which was then converted to CSV with a custom jupyter notebook.\n",
    "* **Fine-Tuning:** In this section we will use QLoRA for fine tuning the *Llama-3.2-3B-Instruct* model.\n",
    "* **Inference:** Lastly we will run our model. I use gradio to provide a GUI for quick and easy usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f288f96-9a67-47a3-871e-72749fcc427c",
   "metadata": {},
   "source": [
    "# Data Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5371da-50d2-4dac-bab8-d257183866dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs\n",
    "#!pip install ipywidgets datasets cryptography torch transformers sentencepiece matplotlib wordcloud\n",
    "\n",
    "# imports\n",
    "import csv\n",
    "import datetime\n",
    "import random\n",
    "from collections import Counter\n",
    "import datasets\n",
    "from cryptography.fernet import Fernet  # to encrypt our texts\n",
    "import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b1a29-1ca6-4d40-95cd-cd1c31920e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MAX_LENGTH = 200  # The length of each chunk\n",
    "DATA_NAME = \"biggestFudge/messagesv3-whatsapp\"  # for uploading\n",
    "ME = \"Guru\" # your name here!\n",
    "base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # for the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc953b-6e59-46f9-a97f-80c90ff182fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HF\n",
    "# Initialize and constants\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "\n",
    "from huggingface_hub import login\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0967c0c-c044-4127-a0cb-c5ea442883d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the csv files containg data \n",
    "# csv must contain 'Chat Session', 'Type', 'Text', 'Message Date'\n",
    "# You can have many more sources of data, I used my personal and Business WhatsApp and Instagram \n",
    "# 'Chat Session' : Name of the chat for e.g. is the chat is with a person named ABC then this colum must contain ABC\n",
    "# 'Type' : Should be ['Incoming', 'Outgoing'] representing is the text was received or was sent by user\n",
    "# 'Text' : The actual text conversation\n",
    "# 'Message Date' : The date of the message\n",
    "txts = []\n",
    "filenames = ['WhatsApp_1.csv', 'WhatsApp_2.csv', 'insta_1.csv']\n",
    "for filename in filenames:\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "        txts.extend(reader)\n",
    "        \n",
    "print(f'Read in a total of {len(txts):,} messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d358b1b-a761-4e97-bc7a-70a09b996d30",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab2db6-da0d-4bdb-a37f-f2f30a7766a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the dummy row from whatsapp\n",
    "INTRO = 'Messages to this chat and calls are now secured with end-to-end encryption'\n",
    "txts = [txt for txt in txts if txt['Text'] != INTRO]\n",
    "print(f'Now a total of {len(txts):,} messages') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb3399-0b73-468e-8d19-9be25d90696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on how does the text looks like\n",
    "txts[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1752a1-59dd-4de3-b054-cc19f9b1d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some cleanup for insta based texts\n",
    "# Here I tried removing some artifacts in text coming from instagram based text.\n",
    "# My name \"Guru Deep\" has been put in cleanup, you can choose anything specific to your usecase\n",
    "\n",
    "# I took help from chatGPT to device this function \n",
    "def clean_text(text: str) -> str: \n",
    "    # Remove http/https links (including common scheme typo https;//)\n",
    "    link_pattern = re.compile(r\"https?[;:][/\\\\]{2}\\S+\", flags=re.IGNORECASE)\n",
    "    text = link_pattern.sub(\"\", text)\n",
    "\n",
    "    # Remove the word before the timestamp, including the two-word \"Guru Deep\"\n",
    "    # Matches examples like:\n",
    "    # \"Guru Deep (Sep 29, 2024 3;09 am)\"\n",
    "    # \"Alex: (Jan 1, 2025 12:05 pm)\"\n",
    "    month = r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\"\n",
    "    datetime_core = rf\"{month}\\s+\\d{{1,2}},\\s+\\d{{4}}\\s+\\d{{1,2}}[;:]\\d{{2}}\\s*(?:am|pm)\"\n",
    "    name_before_dt = re.compile(\n",
    "        rf\"\"\"\n",
    "        (?:\\bGuru\\ Deep|\\b[^\\s()]+)      # \"Guru Deep\" OR a single word just before the (\n",
    "        \\s*[:\\-–—]?\\s*                   # optional punctuation (e.g., colon) between name and (\n",
    "        \\(\\s*{datetime_core}\\s*\\)        # the parenthesized datetime\n",
    "        \"\"\",\n",
    "        flags=re.IGNORECASE | re.VERBOSE,\n",
    "    )\n",
    "    text = name_before_dt.sub(\"\", text)\n",
    "\n",
    "    # Cleanup: collapse spaces and fix spaces before punctuation\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", text)\n",
    "    return text.strip()\n",
    "    \n",
    "\n",
    "# Class Message will be used to create object for every message \n",
    "class Message:\n",
    "\n",
    "    AVOID_WORDS = [\"reagiert\", \"You sent an attachment\", \".gif\", \"Missed voice call\", \"Gefällt\"]\n",
    "    def __init__(self, chat_session, message_type, text, when):\n",
    "        self.name = chat_session\n",
    "        self.sender = self.name if message_type == 'Incoming' else ME    \n",
    "        self.receiver = ME if message_type == 'Incoming' else self.name\n",
    "        self.text = text\n",
    "        if when:\n",
    "            try:\n",
    "                self.when = datetime.datetime.strptime(when, \"%Y-%m-%d %H:%M:%S\")\n",
    "            except ValueError:\n",
    "                # handle unparseable date\n",
    "                self.when = None\n",
    "        else:\n",
    "            self.when = None\n",
    "            \n",
    "        self.massage_text()\n",
    "\n",
    "    def massage_text(self):\n",
    "        \n",
    "        # Some cleanup for instagram\n",
    "        self.text = clean_text(self.text)\n",
    "        \n",
    "        # Replace special characters used in our format for training\n",
    "        self.text = self.text.replace('\\n','  ').replace(':',';').replace('#',';')\n",
    "\n",
    "        # Remove all emojis\n",
    "        emoji_pattern = re.compile(\n",
    "        \"[\" \n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "        \"\\U00002500-\\U00002BEF\"  # Misc symbols\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"\\U0001f926-\\U0001f937\"\n",
    "        \"\\U00010000-\\U0010ffff\"\n",
    "        \"\\u2640-\\u2642\" \n",
    "        \"\\u2600-\\u2B55\"\n",
    "        \"\\u200d\"\n",
    "        \"\\u23cf\"\n",
    "        \"\\u23e9\"\n",
    "        \"\\u231a\"\n",
    "        \"\\ufe0f\"  # Dingbats\n",
    "        \"\\u3030\"\n",
    "        \"]+\", \n",
    "        flags=re.UNICODE)\n",
    "        \n",
    "        self.text = emoji_pattern.sub('', self.text)\n",
    "\n",
    "        \n",
    "        # Indicate if the message is an image\n",
    "        if self.text == '': self.text = '***'\n",
    "    \n",
    "    def should_exclude(self):\n",
    "        if self.when is None:\n",
    "            return True\n",
    "        \n",
    "        if any(word in self.text for word in self.AVOID_WORDS):\n",
    "            return True\n",
    "        return any(ch in self.name for ch in '+&,') or all(ch.isdigit() for ch in self.name)\n",
    "\n",
    "# Create lists of messages\n",
    "messages = [Message(t['Chat Session'], t['Type'], t['Text'], t['Message Date']) for t in txts]\n",
    "messages = [m for m in messages if not m.should_exclude()]\n",
    "print(f'A total of {len(messages):,} messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51103cdd-97ab-492c-8395-bdb078159ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check \n",
    "print(messages[1].sender)\n",
    "print(messages[1].when)\n",
    "print(messages[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edb909-972e-4f69-b359-112cdde4839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize into dict with key = chat name, value = list of messages\n",
    "chats = {}\n",
    "for message in messages:\n",
    "    if message.name not in chats:\n",
    "        chats[message.name] = []\n",
    "    chats[message.name].append(message)\n",
    "\n",
    "# Sort the chats by time\n",
    "for message_list in chats.values():\n",
    "    message_list.sort(key = lambda m: m.when)\n",
    "\n",
    "print(f'{sum([len(v) for v in chats.values()]):,} messages with {len(chats)} people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551f54d-97dd-4c24-b727-d236b7d14207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly removing some chats\n",
    "# I did not want to use group chats as they did not have many messages from me thus I removed them\n",
    "# I needs to customize this list\n",
    "\n",
    "REMOVE_GROUP_CHATS = []\n",
    "print(\"All Conversations before explicit removal: \")\n",
    "print(chats.keys())\n",
    "\n",
    "for rKey in REMOVE_GROUP_CHATS:\n",
    "    if rKey in chats.keys():\n",
    "        chats.pop(rKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb042cc-5124-4b48-8a7f-7da692579c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the chats were removed \n",
    "print(\"All Conversations after explicit removal: \")\n",
    "print(sorted(chats.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9881b3-a142-478f-9130-461ff94482b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only use chats which have minimum of 20 conversations\n",
    "AT_LEAST = 20\n",
    "chats = {name: messages for name, messages in chats.items() if len(messages)>=AT_LEAST}\n",
    "print(f'{sum([len(v) for v in chats.values()]):,} messages with {len(chats)} people')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a60bc42-5dcf-401f-b9db-f90b71519e6c",
   "metadata": {},
   "source": [
    "## Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b68414-a1e0-4b79-9ab2-47af1168227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After all filteration lets analyze the output in graphs\n",
    "messages_cut = []\n",
    "for name, messages in chats.items():\n",
    "    messages_cut.extend(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615493e-874c-48dd-9eae-e4d46778d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "dates = [message.when for message in messages_cut]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.title(\"How many texts I've sent over time\")\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('How many texts');\n",
    "ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda y, p: format(int(y), ',')))\n",
    "_ = ax.hist(dates, bins=20, color='purple', rwidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475e3cb-ce5e-4640-8bfd-4434996b6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "counter = Counter(message.name for message in messages_cut)\n",
    "results = counter.most_common(40)\n",
    "names, counts = zip(*results)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 5))\n",
    "ax.set_ylabel('How many texts');\n",
    "ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda y, p: format(int(y), ',')))\n",
    "plt.xticks(range(len(names)), names, rotation='vertical')\n",
    "_ = ax.bar(names, counts, color ='teal', width = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff7ca7-09f9-441c-a664-e60fd60323a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "text = ' '.join([message.text for message in messages_cut])\n",
    "\n",
    "# Plot\n",
    "wordcloud = WordCloud(max_font_size=60, max_words=100).generate(text)\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c633852c-f967-4464-a20e-7394d3d145b7",
   "metadata": {},
   "source": [
    "## Uploading the data to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de00ea-e442-426d-9aa6-6d4a588cea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd442c-e692-4bf3-96bb-e7c75c08deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document with chat template for llama 3\n",
    "# Please use your own prompt\n",
    "\n",
    "system_prompt = \"\"\"You are Guru in this conversation. Respond only as Guru would in a realistic text message exchange.\n",
    "Write naturally, using the tone, style, and pacing of everyday messaging.\n",
    "Avoid repetition.\n",
    "Never make up facts or invent details.\n",
    "Answer in brief only.\"\"\"\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, name, messages):\n",
    "        self.name = name\n",
    "        self.messages = messages\n",
    "\n",
    "    def to_messages(self):\n",
    "        sys = {\"role\": \"system\", \"content\": system_prompt}\n",
    "        convo = []\n",
    "        for m in self.messages:\n",
    "            role = \"assistant\" if m.sender == ME else \"user\"\n",
    "            convo.append({\"role\": role, \"content\": f\"### {m.sender}: {m.text}\"})\n",
    "        return [sys] + convo\n",
    "\n",
    "    def token_len(self, tokenizer):\n",
    "        prompt = tokenizer.apply_chat_template(self.to_messages(), tokenize=True, add_generation_prompt=False, return_tensors=None)\n",
    "        return len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e138c61-4499-45c9-bba3-504e4ddf95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for name, message_list in tqdm.tqdm(chats.items()):\n",
    "    pointer = 0\n",
    "    while pointer < len(message_list):\n",
    "        size = 1\n",
    "        while pointer + size < len(message_list):\n",
    "            next_doc = Document(name, message_list[pointer:pointer+size+1])\n",
    "            if next_doc.token_len(tokenizer)>=MAX_LENGTH:\n",
    "                break\n",
    "            size += 1\n",
    "        document = Document(name, message_list[pointer:pointer+size])\n",
    "        documents.append(document)\n",
    "        pointer += size\n",
    "print(f\"{len(documents):,} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e771d2-3380-4aa7-b86a-a808efb30e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [doc.to_messages() for doc in documents]\n",
    "lengths = [doc.token_len(tokenizer) for doc in documents]\n",
    "\n",
    "# Count messages: skip the initial system message\n",
    "counts = sum(len(msgs) - 1 for msgs in data)  # -1 for system\n",
    "\n",
    "print(f'There are {counts:,} messages; average {counts/len(documents):.2} messages in each of {len(documents):,} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05fe069-3b5b-45d1-888c-43b7150832af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_xlabel('Number of tokens in a document')\n",
    "ax.set_ylabel('Count of documents')\n",
    "ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda y, p: format(int(y), ',')))\n",
    "l2 = [min(MAX_LENGTH+100,l) for l in lengths]\n",
    "_ = ax.hist(l2, bins=range(0,MAX_LENGTH+50,10), color='darkorange', rwidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82851e26-0d8b-4ed8-a92b-bc0911504758",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42) # for reproducibility\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcfbd55-f4cf-4489-ab66-431bcc5914b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick schema sanity check (raises if anything is off)\n",
    "for i, msgs in enumerate(data):\n",
    "    if not isinstance(msgs, list):\n",
    "        raise TypeError(f\"Document {i} is not a list\")\n",
    "    for j, m in enumerate(msgs):\n",
    "        if not isinstance(m, dict) or \"role\" not in m or \"content\" not in m:\n",
    "            raise TypeError(f\"Doc {i}, message {j} is not a {{'role','content'}} dict: {m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60287798-5240-4602-a346-a3f99e7c84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#serialize each doc to a compact JSON string\n",
    "#(compact separators reduce size; ensure_ascii=False keeps unicode readable pre-encryption)\n",
    "serialized = [\n",
    "    json.dumps(doc, ensure_ascii=False, separators=(\",\", \":\"))\n",
    "    for doc in data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38880f-8167-427a-9836-bc4b76f2f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I chose to encrypt the data\n",
    "# Even though the data is on my private repo on HF I still chose\n",
    "# One layer of added safety\n",
    "\n",
    "key = Fernet.generate_key()\n",
    "print(key)  # NOTE: keep this safe; you'll need it to decrypt later\n",
    "f = Fernet(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0190a-4c80-468e-8efc-e6611662ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted = [f.encrypt(s.encode(\"utf-8\")).decode(\"utf-8\") for s in serialized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e872a-fbff-46f4-857a-5097bdc5c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (same logic as before)\n",
    "split = int(0.95 * len(encrypted))\n",
    "train, test = encrypted[:split], encrypted[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18408856-ad63-4383-becd-ad405f94a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_dict({\"text\": train})\n",
    "test_dataset  = Dataset.from_dict({\"text\": test})\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528ef7e-1bca-4159-bb67-c0a82a40bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HF\n",
    "dataset.push_to_hub(DATA_NAME, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc7e1-4e1c-4828-90b8-80cf4aabf419",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21d0ad-cab2-4dbf-a47e-28e208eda8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from cryptography.fernet import Fernet\n",
    "from getpass import getpass\n",
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "import wandb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1d7a1-41b2-4ead-a5be-698db288557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "DATA_NAME = 'biggestFudge/messagesv3-whatsapp'\n",
    "PROJECT_NAME = 'messages'\n",
    "RUN_NAME = 'v4'\n",
    "MAX_SEQ_LENGTH = 200\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "REFINED_MODEL_NAME = f\"biggestFudge/{PROJECT_NAME}-{RUN_NAME}\"\n",
    "\n",
    "# HYPER-PARAMETERS\n",
    "LORA_ALPHA = 64\n",
    "LORA_R = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "LR_SCHEDULER_TYPE = 'cosine'\n",
    "WEIGHT_DECAY = 0.001\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# OTHER TRAINING CONFIG\n",
    "# Choose your config carefully depending on the scale of your dataset\n",
    "STEPS = 10\n",
    "SAVE_STEPS = 20\n",
    "EVAL_STEPS = 60\n",
    "LOG_TO_WANDB = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ebee4-05f9-4ae3-baef-ed7798af1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up weights and Biases for live plotting\n",
    "wandb_api_key = os.environ['WANDB_API_KEY']\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "wandb.login()\n",
    "\n",
    "# Configure Weights & Biases to record against our project\n",
    "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7d1c7-4b75-4f7d-8acd-45734d56ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_WANDB:\n",
    "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59fa33-c780-4d49-85e5-82737c71554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the dataset from Hugging Face\n",
    "from getpass import getpass\n",
    "encrypted_data = load_dataset(DATA_NAME)\n",
    "\n",
    "# Next, decrypt\n",
    "# It will ask for the key you used during encription \n",
    "key = getpass(\"Enter encryption key\").encode()\n",
    "f = Fernet(key)\n",
    "\n",
    "decrypted = {\"train\": [], \"test\": []}\n",
    "\n",
    "for split_name in [\"train\", \"test\"]:\n",
    "    split = encrypted_data[split_name]\n",
    "    for row in split:\n",
    "        cipher_str = row[\"text\"]                   # base64 string\n",
    "        plain_bytes = f.decrypt(cipher_str.encode(\"utf-8\"))\n",
    "        json_str = plain_bytes.decode(\"utf-8\")\n",
    "        obj = json.loads(json_str)                 # -> list[{\"role\",\"content\"}, ...]\n",
    "        decrypted[split_name].append(obj)\n",
    "\n",
    "# Finally, recreate the dataset\n",
    "train_dataset = Dataset.from_dict({'text':decrypted['train']})\n",
    "test_dataset = Dataset.from_dict({'text':decrypted['test']})\n",
    "data = DatasetDict({'train':train_dataset, 'test':test_dataset})\n",
    "\n",
    "# Quick check\n",
    "print(data)\n",
    "print(data['train'][0])\n",
    "print(data['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b2b57-87dc-412e-a6d0-cf2c5590065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer names\n",
    "base_model_name = BASE_MODEL_NAME\n",
    "refined_model = REFINED_MODEL_NAME\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb65ec-68bf-48fd-988f-e496606720b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_to_prompt(example):\n",
    "    msgs = example[\"text\"]\n",
    "    prompt = llama_tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,  # training on full dialogues\n",
    "    )\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": data[\"train\"].map(render_to_prompt),\n",
    "    \"test\": data[\"test\"].map(render_to_prompt),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e6e66-c5f9-4cbc-b913-e96b0fd0e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['train'][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4d3f9-6395-4566-abb0-8e7884339ca4",
   "metadata": {},
   "source": [
    "## Let the fun begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26e733-c639-4df8-a92f-aaec9b7999d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# Depending on version of trl you may or may not have default datacollator\n",
    "# I used Google collab which had 0.21 and had default data collator\n",
    "\n",
    "# # Data Collator\n",
    "# #from trl import DataCollatorForCompletionOnlyLM\n",
    "# assistant_prefix = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "# data_collator = DataCollatorForCompletionOnlyLM(\n",
    "#     tokenizer=llama_tokenizer,\n",
    "#     response_template=assistant_prefix,\n",
    "# )\n",
    "\n",
    "# # (Optional) quick sanity check: make sure assistant header exists in samples\n",
    "# assert assistant_prefix in data[\"train\"][0][\"text\"], \"Assistant header not found in dataset text\"\n",
    "\n",
    "# --- Training / SFT config ---\n",
    "train_params = SFTConfig(\n",
    "    output_dir=REFINED_MODEL_NAME,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_strategy=\"steps\",                 # <- was evaluation_strategy\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=10,\n",
    "    logging_steps=STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=RUN_NAME,\n",
    "\n",
    "    # SFT-specific bits:\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",             # <- moved here\n",
    "    packing=False,\n",
    "\n",
    "    # Hub\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=REFINED_MODEL_NAME,\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    hub_private_repo=True,\n",
    ")\n",
    "\n",
    "# --- Trainer ---\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    peft_config=peft_parameters,\n",
    "    processing_class=llama_tokenizer,      # <- was tokenizer=\n",
    "    #data_collator=data_collator,\n",
    "    args=train_params,\n",
    ")\n",
    "\n",
    "# Training\n",
    "fine_tuning.train()\n",
    "\n",
    "# Save Model\n",
    "fine_tuning.save_model(refined_model) \n",
    "llama_tokenizer.save_pretrained(refined_model)\n",
    "\n",
    "# --- Push to Hub (adapter + tokenizer) ---\n",
    "fine_tuning.push_to_hub()                  # uses args.* hub settings\n",
    "llama_tokenizer.push_to_hub(REFINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9e2e1-b439-4c55-a86d-023fefde250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_WANDB:\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f38b7e-02b4-4e30-8dba-cc2aec2f667a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e25b6a-ab8b-4bfd-a59a-dda66b678b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, PeftConfig\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c44fe4c-99bb-46fb-ae23-e49ceb55d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "PROJECT_NAME = 'messages'\n",
    "RUN_NAME = 'v4'\n",
    "MODEL_NAME = f\"biggestFudge/{PROJECT_NAME}-{RUN_NAME}\"\n",
    "MAX_LENGTH = 200\n",
    "ME = \"Guru\" # your name here\n",
    "REVISION = None #\"d1a8b673cc3a46bde46c71690ef2d89b54bc1b47\"#None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e963f2-fe37-45d5-a0b5-af4f9a1946da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "if REVISION:\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        MODEL_NAME,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=MAX_LENGTH,\n",
    "    )\n",
    "else:\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        MODEL_NAME,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=MAX_LENGTH,\n",
    "        revision= REVISION   \n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccbcc8-19ef-4180-9a0e-bf0555a7602b",
   "metadata": {},
   "source": [
    "![Gradio Based MOM](../images/digital_guru_stock_discussion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5dbb5-ec6d-4d90-8bf4-0e27db3e2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "# Name for the model's personality\n",
    "BOT_NAME = \"Guru\"\n",
    "\n",
    "# --- System prompt (finish the string!) ---\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You are Guru in this conversation. Respond only as Guru would in a realistic text message exchange.\\n\"\n",
    "#     \"Write naturally, using the tone, style, and pacing of everyday messaging.\\n\"\n",
    "#     \"Avoid repetition at all cost and always answer in brief, no more than few lines.\\n\"\n",
    "#     \"If you are unsure or do not know something, say so clearly.\\n\"\n",
    "#     \"Never make up facts or invent details.\\n\"\n",
    "#     \"Do not break character.\"\n",
    "# )\n",
    "\n",
    "SYSTEM_PROMPT = (\"You are Guru in this conversation. You are snarky. Answer in 2-3 sentences only.\\n\"\n",
    "\"Write naturally, using the tone and style.\\n\"\n",
    "\"Avoid repetition.\\n\"\n",
    "\"Never make up facts or invent details.\\n\"\n",
    "\"Answer in brief only.\")\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def chat_fn(message, history):\n",
    "    # history: list of [user_text, assistant_text] pairs\n",
    "    msgs = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    for pair in history or []:\n",
    "        if isinstance(pair, (list, tuple)) and len(pair) == 2:\n",
    "            u, a = pair\n",
    "            if u:\n",
    "                msgs.append({\"role\": \"user\", \"content\": u})\n",
    "            if a:\n",
    "                msgs.append({\"role\": \"assistant\", \"content\": a})\n",
    "\n",
    "    # current user message\n",
    "    msgs.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # turn into a string prompt using the model's chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        msgs, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs.input_ids.shape[-1]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    reply = tokenizer.decode(outputs[0, input_len:], skip_special_tokens=True).strip()\n",
    "    return reply\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    type=\"tuples\",  # <- classic (message, history) mode\n",
    "    title=f\"Chat with {BOT_NAME}\",\n",
    "    description=\"Type anything and get a reply from Guru.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb11b7-ebbf-445f-9dfc-7fb485ff3712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
