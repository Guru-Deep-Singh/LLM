{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2638696d-79bc-4acc-963c-fdad08c2ef5b",
   "metadata": {},
   "source": [
    "# Medium Blog Summarizer - With OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b87cadb-d513-4303-baee-a37b6f938e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019974d9-f3ad-4a8a-b5f9-0a3719aea2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e793b2-6775-426a-a139-4848291d0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00743dac-0e70-45b7-879a-d7293a6f68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def user_prompt_for_blog(website):\n",
    "    user_prompt = f\"You are looking at a blog titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this blog is as follows; \\\n",
    "please provide a summary of this blog in markdown. \\\n",
    "If it includes references any research papers and github links, please provide their links also.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt\n",
    "\n",
    "\n",
    "def summarize_medium_blog(url, system_prompt, model=\"gpt-4o-mini\"):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "           {\"role\": \"user\", \"content\": user_prompt_for_blog(website)}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b44eee-06fe-4970-94a9-3ab87de45699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "system_prompt_guru = \"You are a scientific assistant. You have to help summarize a blog post.\\\n",
    "Please tell about the most important findings and anything the author emphasizes.\\\n",
    "Please provide all the important links mentioned. \\\n",
    "Give the output in markdown.\"\n",
    "\n",
    "# Blog URL\n",
    "guru_blog = \"https://gurudeep1998.medium.com/w-net-a-deep-model-for-fully-unsupervised-image-segmentation-reproduction-2651540eaed6\"\n",
    "#guru_blog = Website(\"https://medium.com/write-a-catalyst/you-are-fired-now-80458d77205a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a17141c-527a-43ea-ab1c-80d15944210c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Summary of \"W-Net: A Deep Model for Fully Unsupervised Image Segmentation Reproduction\" by Guru Deep Singh\n",
       "\n",
       "## Introduction\n",
       "The blog discusses the implementation of the paper \"W-Net: A Deep Model for Fully Unsupervised Image Segmentation\" by Xide Xia and Brian Kulis. The W-Net architecture consists of two U-net architectures for unsupervised image segmentation, facilitating the reconstruction of images from segmentations. The author and Nadine Duursma aimed to reproduce this model for a student assignment at Delft University of Technology.\n",
       "\n",
       "## W-Net Model Architecture\n",
       "- **Encoder**: Outputs image segmentations from original images.\n",
       "- **Decoder**: Reconstructs images from segmentations.\n",
       "- The architecture has 18 modules, 46 convolutional layers in total, with specific use of depth-wise separable convolutions in many modules for efficiency.\n",
       "\n",
       "### Key Features\n",
       "- Input images are resized to 224x224 pixels.\n",
       "- Utilizes both ReLU and batch normalization.\n",
       "- The model's final layers involve 1x1 convolutions to map feature vectors to the desired number of classes.\n",
       "\n",
       "## Training Methodology\n",
       "- The model was trained on the **PASCAL VOC2012** dataset using a batch size of 10.\n",
       "- Two optimizers were used: one for the encoder (with Soft-N-cut loss) and another for the entire model (with reconstruction loss).\n",
       "- The model implemented dropout to prevent overfitting and utilized a learning rate strategy involving adjustments after certain iterations.\n",
       "\n",
       "## Testing\n",
       "- The model was tested using **BSDS300** and **BSDS500** datasets, and performance was measured using Variation of Information (VI), Probabilistic Rand Index (PRI), and Segmentation Covering (SC).\n",
       "\n",
       "## Reproduction of Results\n",
       "The authors modified three existing GitHub repositories for their reproduction effort:\n",
       "1. **Base Model Repository**: Provided foundational code (Repository [2]).\n",
       "2. **N-cut Loss Repository**: Offered an efficient implementation of N-cut loss (Repository [3]).\n",
       "3. **Metrics Evaluation Repository**: Facilitated performance metric calculations (Repository [4]).\n",
       "\n",
       "## Results and Discussion\n",
       "The authors plotted losses and visualized segmentations throughout training. They observed that while their losses were higher than those reported in the original paper, the visual output was satisfactory, showcasing improved segmentations.\n",
       "\n",
       "### Challenges Encountered\n",
       "Several key parameters were either missing or not clearly mentioned in the original paper, including:\n",
       "- Model initialization\n",
       "- Optimizer specifications\n",
       "- Number of classes (K)\n",
       "\n",
       "These omissions might have impacted the reproduction results.\n",
       "\n",
       "## Conclusion\n",
       "The reproduction of the W-Net model demonstrated promising results despite challenges stemming from incomplete information in the original paper. The authors noted that their visual results were recognizable and closely aligned with the input images, validating the effectiveness of the unsupervised segmentation approach.\n",
       "\n",
       "## Important Links\n",
       "- Original paper: [W-Net: A Deep Model for Fully Unsupervised Image Segmentation](https://arxiv.org/abs/1711.08506) [1].\n",
       "- Repository [2]: [W-Net Implementation in Pytorch](https://github.com/fkodom/wnet-unsupervised-image-segmentation) [2].\n",
       "- Repository [3]: [W-Net Unsupervised Image Segmentation](https://github.com/fkodom/wnet-unsupervised-image-segmentation) [3].\n",
       "- Repository [4]: [BSD500-Segmentation-Evaluator](https://github.com/KuangHaofei/BSD500-Segmentation-Evaluator) [4].\n",
       "- Quantifying Reproducibility: [A Step Toward Quantifying Independently Reproducible Machine Learning Research](http://arxiv.org/abs/1909.06674) [5].\n",
       "\n",
       "---\n",
       "\n",
       "This markdown summary captures the essence of the blog post along with key findings and relevant links to the sources discussed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(summarize_medium_blog(guru_blog, system_prompt_guru, \"gpt-4o-mini\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa5c1e-08b7-48df-92da-21b83fb36b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
