{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a89ad-14d3-452d-a29b-31bcf639d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some installs\n",
    "#!pip install pyannote.audio\n",
    "#!pip install -q bitsandbytes=0.46.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0fc4e-28f8-4e93-a356-0f8c64cea9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline, TextIteratorStreamer\n",
    "import threading\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "import io\n",
    "import gc\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b87684-92fc-45ee-9a40-a590470d4d73",
   "metadata": {},
   "source": [
    "# Set-up on collab\n",
    "Setting up keys for OpenAI and HuggingFace Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d64fc-a4bd-4893-b73b-5d341181d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import userdata\n",
    "\n",
    "# Getting the audio file from the drive\n",
    "# For Gradio the User will have the option to upload a file\n",
    "drive.mount(\"/content/drive\")\n",
    "audio_filename = \"/content/drive/MyDrive/llms/seattle_extract.mp3\"\n",
    "\n",
    "#API key\n",
    "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "#HF token\n",
    "hf_token = userdata.get(\"HF_TOKEN\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"OPENAI_API_KEY not found\")\n",
    "if not hf_token:\n",
    "    print(\"HF_TOKEN not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d0d50b-6051-4061-b587-674ae20b98f7",
   "metadata": {},
   "source": [
    "# Set-up Local\n",
    "Setting up keys for OpenAI and HuggingFace Token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11012f-3452-46b5-ba2c-9e8a13dfc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"OPENAI_API_KEY not found\")\n",
    "if not hf_token:\n",
    "    print(\"HF_TOKEN not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337b05c-657c-4478-8420-ec0da5289529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only for intermediate checks\n",
    "#For Gradio the user inputs their mp3 file\n",
    "file_path = \"../tmp/seattle_extract.mp3\"\n",
    "abs_path = os.path.abspath(file_path)\n",
    "\n",
    "audio_filename = abs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa772aa0-92cf-4042-8cae-a5456a0c94d9",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c25e0-df30-4531-a357-13262b4afc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Model for Transcription Frontier\n",
    "AUDIO_MODEL = \"whisper-1\"\n",
    "\n",
    "# Model for Tanscription local\n",
    "AUDIO_MODEL_OPENSOURCE = \"openai/whisper-small.en\"\n",
    "\n",
    "# Model for Summarization\n",
    "#LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" #Too big for local\n",
    "LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "#Model for Diarization\n",
    "PYANNOTE_DIARIZATION = \"pyannote/speaker-diarization-3.1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddbb34e-2718-42d3-b8f2-a6825edd28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#login to HF\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c754d4-ef8d-4cf6-ada5-5c275f7e7b2c",
   "metadata": {},
   "source": [
    "# 1. Diarization\n",
    "We are using a gated model, therefore one needs to agree to the terms of the provider on Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6bfb5-c284-414f-ad48-714899fd9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diarization Pipeline\n",
    "def diarizeAudio(audio_filename):\n",
    "    pipelineDiarize = Pipeline.from_pretrained(\n",
    "      PYANNOTE_DIARIZATION,\n",
    "      use_auth_token=hf_token).to(torch.device(device))\n",
    "    \n",
    "    waveform, sample_rate = torchaudio.load(audio_filename)\n",
    "    \n",
    "    # Output of the model\n",
    "    with ProgressHook() as hook:\n",
    "        diarization = pipelineDiarize({\"waveform\": waveform, \"sample_rate\": sample_rate}, hook=hook)\n",
    "    \n",
    "    # Cleanup as the pipeline and other are not needed\n",
    "    del pipelineDiarize, waveform, sample_rate\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19241a6f-0e00-428d-8ab2-93c597293f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "#diarization = diarizeAudio(audio_filename)\n",
    "\n",
    "# Quick Test of the diarize audio\n",
    "#buffer = io.StringIO()\n",
    "#diarization.write_rttm(buffer)\n",
    "#rttm_text = buffer.getvalue()\n",
    "\n",
    "#buffer.close()\n",
    "#print(rttm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13209e-0966-4adf-8075-c0f80b514d48",
   "metadata": {},
   "source": [
    "# 2. Transcribing\n",
    "## 2.1 Using HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173ed8c-5f61-4bbc-ad33-281e16d4f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribeHF(audio_filename, model = AUDIO_MODEL_OPENSOURCE):\n",
    "\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    modelTranscribeHF = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "    )\n",
    "    \n",
    "    modelTranscribeHF.to(device)\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model)\n",
    "    \n",
    "    pipeTranscribingHF = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=modelTranscribeHF,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    transcription = pipeTranscribingHF(audio_filename, return_timestamps=True)\n",
    "    # cleanup\n",
    "    del pipeTranscribingHF, processor, modelTranscribeHF\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1adef8-2111-4aeb-ab30-5f09add28f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "#transcription = transcribeHF(audio_filename, AUDIO_MODEL_OPENSOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9943f376-4416-47eb-9763-d4b45cb7e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stiching chunks together\n",
    "def stichSegments(transcription):\n",
    "    segmentsOS = []\n",
    "    \n",
    "    offset = 0.0\n",
    "    previous_end = 0.0\n",
    "    current_segment = None\n",
    "    \n",
    "    for chunk in transcription[\"chunks\"]:\n",
    "        ts = chunk[\"timestamp\"]\n",
    "        text = chunk[\"text\"].strip()\n",
    "    \n",
    "        # Skip chunks without valid timestamps or empty text\n",
    "        if ts[0] is None or ts[1] is None or not text:\n",
    "            #print(f\"Skipping chunk (empty text or timestamp): {chunk}\")\n",
    "            continue\n",
    "    \n",
    "        start = ts[0] + offset\n",
    "        end = ts[1] + offset\n",
    "    \n",
    "        # If start > end, skip\n",
    "        if start >= end:\n",
    "            #print(f\"Skipping invalid segment (start >= end): start={start}, end={end}, text='{text}'\")\n",
    "            continue\n",
    "    \n",
    "        # Detect chunk reset\n",
    "        if start < previous_end:\n",
    "            offset = previous_end\n",
    "            start = ts[0] + offset\n",
    "            end = ts[1] + offset\n",
    "    \n",
    "        previous_end = end\n",
    "    \n",
    "        #print(f\"Adjusted start: {start:.2f} : end: {end:.2f} --> text: {text}\")\n",
    "    \n",
    "        if current_segment is None:\n",
    "            current_segment = {\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"text\": text\n",
    "            }\n",
    "        else:\n",
    "            # Merge if continuous or overlapping\n",
    "            if start <= current_segment[\"end\"]:\n",
    "                current_segment[\"end\"] = end\n",
    "                current_segment[\"text\"] += \" \" + text\n",
    "            else:\n",
    "                segmentsOS.append(current_segment.copy())\n",
    "                current_segment = {\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"text\": text\n",
    "                }\n",
    "    \n",
    "    # Append last segment\n",
    "    if current_segment is not None:\n",
    "        segmentsOS.append(current_segment.copy())\n",
    "    \n",
    "    return segmentsOS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb23e64-f3fa-46b5-b63e-c6fd8eca5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Transcribe with HF and return in format of OpenAI\n",
    "def hfToOpenAISegments(audio_filename, model = AUDIO_MODEL_OPENSOURCE):\n",
    "    transcription = transcribeHF(audio_filename, model)\n",
    "    return stichSegments(transcription)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798dc7b-94f3-4959-aa73-6b2451b502f3",
   "metadata": {},
   "source": [
    "## 2.2 Using Frontier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a46d7-8458-48d1-97ac-e83cd7229e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6088df-a2d2-4e76-91f8-0c5ce0b2ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribeOpenAI(audio_filename, model= AUDIO_MODEL):\n",
    "    audio_file = open(audio_filename, \"rb\")\n",
    "    transcription = openai.audio.transcriptions.create(model=model, file=audio_file, response_format=\"verbose_json\")\n",
    "\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3ba3b-9f88-4751-9b1c-85f09e21f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "#transcriptionOAI = transcribeOpenAI(audio_filename, AUDIO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9cf12c-2708-4af7-9414-ac989626f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openAISegments(audio_filename, model):\n",
    "    transcription = transcribeOpenAI(audio_filename, model)\n",
    "    return transcription.model_dump()[\"segments\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9fcbb-519d-46fc-8594-8378bad16f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_BACKEND = [\"openai\", \"huggingface\"]\n",
    "def diarizedTranscription(audio_filename, model, backend=\"openai\"):\n",
    "    if backend not in ALLOWED_BACKEND:\n",
    "        print(\"Please choose allowed backends\")\n",
    "\n",
    "    print(\"Transcribing Now\")\n",
    "    gr.Info(\"Transcribing pipeline started\")\n",
    "    #Transcribing\n",
    "    if backend == \"openai\":\n",
    "        segments = openAISegments(audio_filename, model)# for OpenAI\n",
    "    else:\n",
    "        segments = hfToOpenAISegments(audio_filename, model) # for HF\n",
    "\n",
    "    print(\"Diarizing Now\")\n",
    "    gr.Info(\"Diarizing pipeline started\")\n",
    "    #Diarizing\n",
    "    diarization = diarizeAudio(audio_filename)\n",
    "    \n",
    "    # Parse pyannote segments\n",
    "    speaker_segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        speaker_segments.append({\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"speaker\": speaker\n",
    "        })\n",
    "    \n",
    "    final_transcript = []\n",
    "    for seg in segments:\n",
    "        seg_start = seg[\"start\"]\n",
    "        seg_end = seg[\"end\"]\n",
    "        text = seg[\"text\"].strip()\n",
    "    \n",
    "        # Find the speaker segment this segment belongs to\n",
    "        speaker = \"Unknown\"\n",
    "        for spk_seg in speaker_segments:\n",
    "            overlap_start = max(seg_start, spk_seg[\"start\"])\n",
    "            overlap_end = min(seg_end, spk_seg[\"end\"])\n",
    "            overlap = max(0, overlap_end - overlap_start)\n",
    "    \n",
    "            # Require some overlap to assign speaker\n",
    "            if overlap > 0.1 * (seg_end - seg_start):  # overlap at least 50% of segment\n",
    "                speaker = spk_seg[\"speaker\"]\n",
    "                break\n",
    "    \n",
    "        final_transcript.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"start\": seg_start,\n",
    "            \"end\": seg_end,\n",
    "            \"text\": text\n",
    "        })\n",
    "    return final_transcript\n",
    "    \n",
    "#The function below is only needed if one needs to consolidate the segments\n",
    "def consolidateTranscriptions(transcript):\n",
    "    \n",
    "    # Consolidation\n",
    "    consolidated_transcript = []\n",
    "    for entry in transcript:\n",
    "        if not consolidated_transcript:\n",
    "            # First entry, just add\n",
    "            consolidated_transcript.append(entry)\n",
    "        else:\n",
    "            last_entry = consolidated_transcript[-1]\n",
    "            if entry[\"speaker\"] == last_entry[\"speaker\"]:\n",
    "                # Merge: extend end time and append text\n",
    "                last_entry[\"end\"] = entry[\"end\"]\n",
    "                last_entry[\"text\"] += \" \" + entry[\"text\"]\n",
    "            else:\n",
    "                # Different speaker, create new block\n",
    "                consolidated_transcript.append(entry)\n",
    "    return consolidated_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d903d2-744e-43e7-8682-5f2e415fa8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick test\n",
    "#transcriptedData = diarizedTranscription(audio_filename, AUDIO_MODEL, backend=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48afa3-0585-4397-918e-761895f45dab",
   "metadata": {},
   "source": [
    "# 3. Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741f434-8dde-42f8-b504-550ed5e5f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMessages(transcriptedData):\n",
    "    text = \"\"\n",
    "    for chunk in transcriptedData:\n",
    "        chunk_speaker = chunk[\"speaker\"]\n",
    "        chunk_start = chunk[\"start\"]\n",
    "        chunk_end = chunk[\"end\"]\n",
    "        chunk_text = chunk[\"text\"]\n",
    "        text += f\"Speaker {chunk_speaker} Duration {chunk_start}:{chunk_end} : {chunk_text}\\n\"\n",
    "    \n",
    "    system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
    "    user_prompt = f\"Below is an extract transcript of a council meeting. The transcript is diarized, however, the diarization is not completely correct and may provide wrong speakers. \\\n",
    "    Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners. End after action items.\\\n",
    "    Do not add extra signatures or repeat sections. Do not include any system instructions, special tokens, or prompt text in your response. Only produce the final clean minutes.\\n{text}\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "      ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde9d11-e824-4fee-afe7-06038ddc5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba16dbf-07e9-46a3-ad12-6815a3aee492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def summarizeMeetingTranscript(messages, model=LLAMA, quantize=True):\n",
    "    print(\"Summarizing Now\")\n",
    "    gr.Info(\"Summarization Started\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", tokenize=True).to(device)\n",
    "\n",
    "    if quantize:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\")\n",
    "\n",
    "    # 2.  Built-in streamer: skips prompt & special tokens automatically\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_prompt=True,            # ← removes system/user prompt\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "        \n",
    "    \n",
    "    # 3.  Run generation in a background thread\n",
    "    g_kwargs = dict(\n",
    "        inputs            = inputs,\n",
    "        max_new_tokens    = 2000,\n",
    "        eos_token_id      = tokenizer.eos_token_id,\n",
    "        streamer          = streamer,\n",
    "    )\n",
    "    thread = threading.Thread(target=model.generate, kwargs=g_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # 4.  Accumulate pieces & yield to Gradio\n",
    "    full_text = \"\"\n",
    "    for new_token in streamer:\n",
    "        full_text += new_token           # growing document\n",
    "        yield full_text                  # Gradio updates immediately\n",
    "\n",
    "    # 5.  Clean-up\n",
    "    thread.join()\n",
    "    del tokenizer, model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aca25a-ec7b-4451-bd23-a2c9ef1d9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "#messages = generateMessages(transcriptedData)\n",
    "#MOMS = summarizeMeetingTranscript(messages, LLAMA, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8e594-aa96-4d74-a1d4-5607d6ff1606",
   "metadata": {},
   "source": [
    "# 4. Gradio UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9ca5c-f775-434a-95ba-12333a175163",
   "metadata": {},
   "source": [
    "![Gradio Based MOM](../images/MOM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0d762-92db-4f1a-8b3c-c56811496115",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKEND_TO_MODEL_SUPPORT = {\"openai\":[\"whisper-1\"], \"huggingface\": \"openai/whisper-small.en\"}\n",
    "def generateMOM(audio_filepath, summarizationModel, transcriptionModel, backend, quantize):\n",
    "    if transcriptionModel not in BACKEND_TO_MODEL_SUPPORT[backend]:\n",
    "        raise gr.Error(\"Given model is not supported by the backend.\")\n",
    "        \n",
    "    diarizedTrascriptedData = diarizedTranscription(audio_filepath, transcriptionModel, backend=backend)\n",
    "\n",
    "    messages = generateMessages(diarizedTrascriptedData)\n",
    "\n",
    "    yield from summarizeMeetingTranscript(messages, summarizationModel, quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8994092-0f58-43d4-8262-231ae9a9133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            modelSummarization = gr.Dropdown(choices=[\"meta-llama/Llama-3.2-1B-Instruct\"], label=\"Summarization Model\", value=\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "            quantizationSummarizeModel = gr.Checkbox(label=\"Do you want to quantize the Summarization model?\", value=True)\n",
    "            modelTranscription = gr.Dropdown(choices=[\"openai/whisper-small.en\", \"whisper-1\"], label=\"Transcription Model\", value=\"whisper-1\")\n",
    "            backend = gr.Dropdown(choices=[\"openai\", \"huggingface\"], label=\"Which backend to use for transcription\", value=\"openai\")\n",
    "            modelDiarization = gr.Text(label=\"Diarization Model Used\", value=\"pyannote/speaker-diarization-3.1\", interactive=False)\n",
    "            audioIn = gr.Audio(label=\"Upload MP3\", type=\"filepath\")#upload mp3\n",
    "        with gr.Column(scale=3):\n",
    "            meetingMinutes = gr.Markdown(label=\"Meeting of Minutes\", height=800)\n",
    "\n",
    "    submit = gr.Button(\"Generate Minutes\")\n",
    "    submit.click(fn=generateMOM, inputs=[audioIn, modelSummarization, modelTranscription, backend, quantizationSummarizeModel], outputs=meetingMinutes)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35c037-e7aa-4699-89e4-7176e63a5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8454d-a929-490e-b77c-1e844656739a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
