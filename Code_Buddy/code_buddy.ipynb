{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a21f62-76f7-484d-9a28-3d2740c6ae85",
   "metadata": {},
   "source": [
    "# Code Buddy: Your Technical Code Assistant\n",
    "Made by: Guru Deep Singh\n",
    "\n",
    "Code Buddy is an LLM powered assistant allowing you to ask it questions regarding your coding language issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facd1fe8-928a-4919-9704-9ede351c556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eff0906-4a0a-4c01-9d49-4f534a8cf3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for creating a LLM powered assistant\n",
    "\n",
    "class CodeBuddy:\n",
    "    ALLOWED_BACKENDS = {\"ollama\", \"openai\"}\n",
    "    def __init__(self, backend=\"ollama\", modelName= \"llama3.2\"):\n",
    "        if backend not in self.ALLOWED_BACKENDS:\n",
    "            raise ValueError(f\"Invalid backend '{backend}'. Allowed options are: {', '.join(self.ALLOWED_BACKENDS)}\")\n",
    "           \n",
    "        self.backend = backend\n",
    "        self.modelName = modelName\n",
    "        print(f\"Code Buddy using {self.backend} backend and {modelName} model\")\n",
    "        self.__initialize()\n",
    "\n",
    "    def __initialize(self):\n",
    "        if self.backend == \"openai\":\n",
    "            load_dotenv(override=True)\n",
    "            apiKey = os.getenv('OPENAI_API_KEY')\n",
    "            if not apiKey:\n",
    "                print(\"No API key was found\")\n",
    "\n",
    "            else:\n",
    "                self.__requestHandler = OpenAI()\n",
    "\n",
    "        elif self.backend == \"ollama\":\n",
    "            # Setting up Constants\n",
    "            self._OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "            self._HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "            self.__requestHandler = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "            \n",
    "    def __nonStreamCall(self,userPrompt, systemPrompt):\n",
    "        response = self.__requestHandler.chat.completions.create(\n",
    "            model = self.modelName,\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": systemPrompt},\n",
    "                {\"role\": \"user\", \"content\": userPrompt}\n",
    "                ])\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def __streamCall(self,userPrompt, systemPrompt):\n",
    "        stream = self.__requestHandler.chat.completions.create(\n",
    "                model = self.modelName,\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": systemPrompt},\n",
    "                    {\"role\": \"user\", \"content\": userPrompt}\n",
    "                ],\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "        for chunk in stream:\n",
    "            response = chunk.choices[0].delta\n",
    "            if hasattr(response, \"content\"):\n",
    "                yield response.content\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, userPrompt, stream = False, \n",
    "            systemPrompt =\"You are a helpful and informed coding agent.\\\n",
    "            You are given a piece of code. You have to check if the code is correct or is incorrect.\\\n",
    "            You need to explain the code in beginner friendy way.\\\n",
    "            You are also allowed to give suggestions on improvement of code for runtime optimization.\\\n",
    "            Give your answer in Markdown.\" ):\n",
    "        \n",
    "        if len(systemPrompt.strip()) != 0 and len(userPrompt.strip()) != 0:\n",
    "            if not stream:\n",
    "                return self.__nonStreamCall(userPrompt, systemPrompt)\n",
    "\n",
    "            else:\n",
    "                return self.__streamCall(userPrompt, systemPrompt)\n",
    "                \n",
    "            \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a25fac-691c-4faf-88fa-f005d1363d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for streaming\n",
    "def stream_markdown(generator):\n",
    "    buffer = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for part in generator:\n",
    "        buffer += str(part)\n",
    "        cleaned = buffer.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(cleaned), display_id=display_handle.display_id)\n",
    "\n",
    "def stream(generator):\n",
    "    for part in generator:\n",
    "        print(part, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210123a-959d-4bf3-9def-777ec0d5362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "# Assistant powered by OpenAI API \n",
    "myBuddy = CodeBuddy(\"openai\", \"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973cfa1-6037-4290-bb66-f8ec34904214",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = myBuddy.run(userPrompt = question, stream=False)\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61acef51-cf17-4d79-8104-dcc542975dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_markdown(myBuddy.run(userPrompt=question, stream=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915241ab-14ee-4108-b78e-994ea0262bf4",
   "metadata": {},
   "source": [
    "# Ollama based Code Buddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3815e94-9651-49cf-9157-a22d57f3e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assistant powered by Ollama \n",
    "myBuddyOllama = CodeBuddy(\"ollama\", \"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89093d17-d1be-43bf-bbf5-03b1429ed627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling model\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda418e-06ba-4ab8-9d40-1e160e897d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputOllama = myBuddyOllama.run(userPrompt = question, stream=False)\n",
    "display(Markdown(outputOllama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818988b0-2b5e-40ba-ae21-dd82ebd57e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_markdown(myBuddyOllama.run(userPrompt=question, stream=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a5229-cf8c-478a-828b-21ece8e8b212",
   "metadata": {},
   "source": [
    "# Creating a GUI with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea05cdbd-a58f-4c50-8004-dd79565e4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1673bd-80ce-468a-86f3-3360ff2698dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio function call \n",
    "def runModel(model, userPrompt):\n",
    "    if model == \"llama3.2\":\n",
    "        !ollama pull llama3.2\n",
    "        stream = CodeBuddy(\"ollama\", model).run(userPrompt, True)\n",
    "    elif model == \"gpt-4o-mini\":\n",
    "        gr.Warning(\"Paid Service being Used\")\n",
    "        stream = CodeBuddy(\"openai\", model).run(userPrompt, True)\n",
    "    else:\n",
    "        raise ValueError(\"Model not supported\")\n",
    "    result = \"\"\n",
    "    for part in stream:\n",
    "        result += str(part)\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07601c1c-c914-4c51-b4c9-22990d5d1260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface = gr.Interface(fn=runModel, inputs=[gr.Dropdown([\"llama3.2\", \"gpt-4o-mini\"], label=\"Model\", value=\"llama3.2\"), gr.Textbox(label=\"Enter your Coding question here\")],\n",
    "                        outputs=gr.Markdown(label=\"Response:\"),\n",
    "                        flagging_mode=\"never\")\n",
    "interface.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42024e70-2fc7-4979-b5b7-3b3c993ca660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
